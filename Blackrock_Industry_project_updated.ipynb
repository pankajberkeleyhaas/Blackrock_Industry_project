{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100GO_bonds_coverage.csv\r\n",
      "aladdin_index_data_sample_cols-copy1.ipynb\r\n",
      "Blackrock_Project_Code (1).ipynb\r\n",
      "Carbon_Data_Analysis_MUB_Bmk.ipynb\r\n",
      "Carbon_Data_by_GDP.csv\r\n",
      "Carbon_Data_Carbon_Monitor_BEA_Analysis_MUB_Bmk_Working.ipynb\r\n",
      "Carbon_Data_EIA_MUB_Bmk_Per_Capita_Working.ipynb\r\n",
      "Carbon_Data_EIA_MUB_Bmk_Working.ipynb\r\n",
      "Carbon_Per_Capita.csv\r\n",
      "charts&graphs.ipynb\r\n",
      "Coding_stack.ipynb\r\n",
      "Coding Stack_version2_with trend signal.ipynb\r\n",
      "Coding Stack_version3_with zscored trend signal.ipynb\r\n",
      "Consumption Energy Data.xlsx\r\n",
      "Daily_carbonmonitor.csv\r\n",
      "Datapoint_name_Revenue_bonds.csv\r\n",
      "esg_data_prism_example.ipynb\r\n",
      "ESG_Muni_MFRA.ipynb\r\n",
      "ESG_Muni_Sample-Copy2.ipynb\r\n",
      "ESG_Muni_Sample.ipynb\r\n",
      "ESG_Muni_Sample-Orig.ipynb\r\n",
      "ESG_Signal_Framework_Pankaj.ipynb\r\n",
      "Green_Consumption_percent_signal.ipynb\r\n",
      "Green_Consumption_percent_signal_with_q5_minus_benchmark.ipynb\r\n",
      "Green_percent.csv\r\n",
      "Green_percent_debt.ipynb\r\n",
      "Green_Percent_Production.csv\r\n",
      "Green_Policy_Rank_Dataset.csv\r\n",
      "Green_policy_signal.ipynb\r\n",
      "Green_policy_signal_with_benchmark_minus_q1-Copy1.ipynb\r\n",
      "Green_policy_signal_with_benchmark_minus_q1.ipynb\r\n",
      "Green_Production_percent_signal.ipynb\r\n",
      "MFRA_datapoints_GO_Bond_Signals.ipynb\r\n",
      "MFRA_datapoints_Revenue_Bond_Signals.ipynb\r\n",
      "MFRA_GO_Bonds.ipynb\r\n",
      "Mub_MFRA\r\n",
      "Mun_MFRA.pkl\r\n",
      "Quarterly_GDP.csv\r\n",
      "Quarterly._GDP_upd.csv\r\n",
      "Testing Coverage logic.ipynb\r\n",
      "Testing State Level Signal.ipynb\r\n",
      "Untitled1.ipynb\r\n",
      "Untitled2.ipynb\r\n",
      "Untitled3.ipynb\r\n",
      "Untitled4.ipynb\r\n",
      "Untitled5.ipynb\r\n",
      "Untitled.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os\n",
    "from google.colab import drive \n",
    "# drive.mount('/content/drive') ## use this when you need to mount the google drive\n",
    "#Importing data for ICE index \n",
    "import pickle \n",
    "# Importing data for future manipulations \n",
    "from google.colab import files \n",
    "# uploaded = files.upload() ## use this when you need to upload the files in the workspace\n",
    "\n",
    "# importing matlplotlib for plotting purposes\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "from imblearn.pipeline import Pipeline \n",
    "from sklearn.linear_model import LinearRegression \n",
    "from matplotlib import pyplot \n",
    "\n",
    "# After Rescaling Most important Features\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "import statsmodels.api as sm \n",
    "from collections import defaultdict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Data below and saving for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some changes to be done in the code to use this\n",
    "#Importing data for ICE index \n",
    "\n",
    "import pickle \n",
    "\n",
    "if 'ICE_dataset.pkl' in listfiles:\n",
    "  pkl_file = open('ICE_dataset.pkl', 'rb')\n",
    "  ICE_dataset = pickle.load(pkl_file)\n",
    "  pkl_file.close()\n",
    "\n",
    "else:\n",
    "  ICE_dataset = {} \n",
    "  for i in listfiles: \n",
    "    ICE_dataset[i[5:13]] = pd.read_excel(root_drive+'/ICE/'+i,skiprows=[0]) \n",
    "  # write python dict to a file\n",
    "  output = open('ICE_dataset.pkl', 'wb') \n",
    "  pickle.dump(ICE_dataset, output) \n",
    "  output.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct Green Energy signal \n",
    "def construct_green_energy_signal(data, columns = ['date','state_ticker','Green_percent'],new_column): \n",
    "    Pivot_scores_df = data[columns].copy() \n",
    "    Green_energy_scores = get_xs_scores(Pivot_scores_df.pivot_table(index = 'date', columns = 'state_ticker', values = columns[2])) \n",
    "    data.set_index(list(columns[:2]), inplace = True) \n",
    "    data[new_column] = Green_energy_scores.stack() \n",
    "    return data \n",
    "\n",
    "def get_xs_scores(in_data, wins_upp = 3, wins_low = -3, percent = False): \n",
    "    # winsorize \n",
    "    in_data = in_data.clip(in_data.mean(1) + wins_low * in_data.std(1), in_data.mean(1) + wins_upp * in_data.std(1), axis = 0) \n",
    "    # XS z-score \n",
    "    if percent: \n",
    "        return in_data.rank(1, pct=True).subtract(in_data.rank(1, pct=True).mean(1), 0) \n",
    "    else: \n",
    "        return in_data.subtract(in_data.mean(1), 0).divide(in_data.std(1), 0) \n",
    "    \n",
    "def create_quintile(data, signal): \n",
    "    data[signal] = data[signal].fillna(0.0)  \n",
    "    data['quintile'] = data.groupby(level = 0, group_keys = False).apply(lambda x: pd.qcut(x[signal].rank(method='first'), 5, labels = range(1, 6))).astype(int)     \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quintile_analysis_benchmark_remove_quintile(data, signal, bmk_ret, quintile): \n",
    "    data = create_quintile(data, signal) \n",
    "    \n",
    "    # removing securities in the first quintile \n",
    "    data.drop(data[data['quintile'] == quintile].index, inplace = True) \n",
    "    data.reset_index(inplace = True)\n",
    "    data = construct_green_energy_signal(data,signal) \n",
    "    data = create_quintile(data, signal) \n",
    "    tot_ret_q = data.reset_index(level=0).groupby(['date', 'quintile']).apply(lambda x: (x['bmk_wgt'] * x['next_ret']).sum() / x.loc[x['next_ret'].notnull(), 'bmk_wgt'].sum()).unstack() \n",
    "    tot_ret_q['bmk'] = bmk_ret \n",
    "    \n",
    "    signal_stats = pd.DataFrame(index = ['Total Return', 'Total Vol', 'SR', 'Alpha', 'Beta', 'Beta Adjusted IR'], columns = [1, 2, 3, 4, 5,'bmk']) \n",
    "    signal_stats.loc['Total Return'] = tot_ret_q.mean() * 12 \n",
    "    signal_stats.loc['Total Vol'] = tot_ret_q.std() * np.sqrt(12) \n",
    "    # assume risk free rate of 0... need to add rf rate \n",
    "    signal_stats.loc['SR'] = (tot_ret_q.mean() * 12) / (tot_ret_q.std() * np.sqrt(12)) \n",
    "    resid_ret_q = pd.DataFrame(index = tot_ret_q.index, columns = [1, 2, 3, 4, 5,'bmk']) \n",
    "    for i in range(1, 6): \n",
    "        tmp_reg = sm.OLS(tot_ret_q[i], sm.add_constant(tot_ret_q['bmk']), missing = 'drop').fit() \n",
    "        resid_ret_q[i] = tmp_reg.resid + tmp_reg.params['const'] \n",
    "        signal_stats.loc['Alpha', i] = resid_ret_q[i].mean() * 12 \n",
    "        signal_stats.loc['Beta', i] = tmp_reg.params['bmk'] \n",
    "        signal_stats.loc['Beta Adjusted IR', i] = (resid_ret_q[i].mean() * 12) / (resid_ret_q[i].std() * np.sqrt(12))         \n",
    "    return tot_ret_q, resid_ret_q, signal_stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_plots(data,label = 'Index',ind_lvl = 'Industry Lvl 3 Desc',val = 'CA'):\n",
    "  \n",
    "  data_ind_l3_ret = data[['As of Date',ind_lvl,'% Mkt Value']].copy() \n",
    "  data_ind_l3_ret_grp = data_ind_l3_ret.groupby(['As of Date',ind_lvl]).sum() \n",
    "  data_ind_l3_ret_grp = data_ind_l3_ret_grp.reset_index( level = [0,1] ) \n",
    "  data_ind_l3_ret_grp_stacked = data_ind_l3_ret_grp.set_index(['As of Date', ind_lvl]).unstack([ind_lvl]) \n",
    "  data_ind_l3_ret_grp_stacked = data_ind_l3_ret_grp_stacked.reindex(columns = sorted(data_ind_l3_ret_grp_stacked.columns, key = lambda x: x[::-1])) \n",
    "  data_ind_l3_ret_grp_stacked.columns = data_ind_l3_ret_grp_stacked.columns.droplevel() \n",
    "  data_ind_l3_ret_grp_stacked.reset_index(inplace = True) \n",
    "  data_ind_l3_ret_grp_stacked.plot(x = 'As of Date', kind = 'bar', stacked = True, title = 'Stacked Bar '+ val +'_'+ label + ind_lvl)  \n",
    "  ax1 = plt.axes()\n",
    "  x_axis = ax1.axes.get_xaxis()\n",
    "  x_axis.set_visible(False)\n",
    "  plt.show()\n",
    "  return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bar plots in the data \n",
    "def barplot():\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_statistics(model,X,y):\n",
    "    yhat = model.predict(X)\n",
    "    SS_Residual = sum((y - yhat)**2)       \n",
    "    SS_Total = sum((y-np.mean(y))**2)     \n",
    "    r_squared = 1 - (float(SS_Residual))/SS_Total\n",
    "    adjusted_r_squared = 1 - (1-r_squared)*(len(y)-1)/(len(y)-X.shape[1]-1)\n",
    "    print(r_squared, adjusted_r_squared) \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_org_years(x):\n",
    "  if(0<=x<10):\n",
    "    return '1-10'\n",
    "  elif(10<=x<15):\n",
    "    return '10-15'\n",
    "  elif(15<=x<20):\n",
    "    return '15-20'\n",
    "  elif(20<=x<25):\n",
    "    return '20-25'\n",
    "  elif(25<=x<30):\n",
    "    return '25-30'\n",
    "  elif(30<=x<35):\n",
    "    return '30-35'\n",
    "  elif(35<=x<40):\n",
    "    return '35-40'\n",
    "  elif(40<=x<45):\n",
    "    return '40-45'\n",
    "  elif(45<=x<50):\n",
    "    return '45-50'\n",
    "  elif(50<=x<55):\n",
    "    return '50-55'\n",
    "\n",
    "def liquidity_analysis(data):\n",
    "  data['Diff_years'] = (data['Maturity Date'] -  data['As of Date'])/365 \n",
    "  data['Diff_years'] = data['Diff_years'].dt.days \n",
    "  data['Diff_years_interval'] = data['Diff_years'].apply(lambda x : diff_org_years(x)) \n",
    "  data['Maturty_time_weight'] = data['% Mkt Value']*data['Diff_years'] \n",
    "  data_grp_interval_sum = data[data['As of Date']==data['As of Date'][0]].groupby(['Diff_years_interval']).sum() \n",
    "  print(data_grp_interval_sum.loc[:,['% Mkt Value','Face Value_LOC','Maturty_time_weight']]) \n",
    "  return\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
